{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "froked from [fran√ßois chollet's](https://colab.research.google.com/drive/1UCJt8EYjlzCs1H1d1X0iDGYJsHKwu-NO?fbclid=IwAR2Gpas2wWafFbBiNKQyp4RewKlRaw9QBYnC_nkn1Mrao5UIaUHOas-_Y3s#scrollTo=MfO_uy61upRm) notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/tf2.0/derivative 1.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.random.normal(shape=(2, 2))\n",
    "b = tf.random.normal(shape=(2, 2))\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(a)  # Start recording the history of operations applied to `a`\n",
    "    c = tf.sqrt(tf.square(a) + tf.square(b))  # Do some math using `a`\n",
    "    # What's the gradient of `c` with respect to `a`?\n",
    "    dc_da = tape.gradient(c, a)\n",
    "    print(dc_da)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, variables are watched automatically, so you don't need to manually watch them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.Variable(a)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    c = tf.sqrt(tf.square(a) + tf.square(b))\n",
    "    dc_da = tape.gradient(c, a)\n",
    "    print(dc_da)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can compute higher-order derivatives by nesting tapes:\n",
    "\n",
    "\n",
    "<img src=\"../images/tf2.0/derivative 2.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as outer_tape:\n",
    "    with tf.GradientTape() as tape:\n",
    "        c = tf.sqrt(tf.square(a) + tf.square(b))\n",
    "        dc_da = tape.gradient(c, a)\n",
    "    d2c_da2 = outer_tape.gradient(dc_da, a)\n",
    "    print(d2c_da2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# An end-to-end example: linear regression\n",
    "\n",
    "So far you've learned that TensorFlow is a Numpy-like library that is GPU or TPU accelerated, with automatic differentiation. Time for an end-to-end example: let's implement a linear regression, the FizzBuzz of Machine Learning.\n",
    "\n",
    "For the sake of demonstration, we won't use any of the higher-level Keras components like Layer or MeanSquaredError. Just basic ops.\n",
    "\n",
    "*note the usage of [assign_sub](https://www.tensorflow.org/api_docs/python/tf/Variable#assign_sub) to update the gradients of the weights an bias tensors*\n",
    "\n",
    "<img src=\"../images/tf2.0/neuron_model.jpeg\" width=\"600\">\n",
    "\n",
    "<img src=\"../images/tf2.0/mse.jpg\" width=\"500\">\n",
    "\n",
    "<img src=\"../images/tf2.0/gradients.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Prepare a dataset.\n",
    "num_samples = 10000\n",
    "negative_samples = np.random.multivariate_normal(\n",
    "    mean=[0, 3], cov=[[1, 0.5],[0.5, 1]], size=num_samples)\n",
    "positive_samples = np.random.multivariate_normal(\n",
    "    mean=[3, 0], cov=[[1, 0.5],[0.5, 1]], size=num_samples)\n",
    "features = np.vstack((negative_samples, positive_samples)).astype(np.float32)\n",
    "labels = np.vstack((np.zeros((num_samples, 1), dtype='float32'),\n",
    "                    np.ones((num_samples, 1), dtype='float32')))\n",
    "\n",
    "plt.scatter(features[:, 0], features[:, 1], c=labels[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "\n",
    "input_dim = 2\n",
    "output_dim = 1\n",
    "learning_rate = 0.01\n",
    "\n",
    "# This is our weight matrix\n",
    "w = tf.Variable(tf.random.uniform(shape=(input_dim, output_dim)))\n",
    "# This is our bias vector\n",
    "b = tf.Variable(tf.zeros(shape=(output_dim,)))\n",
    "\n",
    "def compute_predictions(features):\n",
    "    return tf.matmul(features, w) + b\n",
    "\n",
    "def compute_loss(labels, predictions):\n",
    "    return tf.reduce_mean(tf.square(labels - predictions))\n",
    "\n",
    "def train_on_batch(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = compute_predictions(x)\n",
    "        loss = compute_loss(y, predictions)\n",
    "        # Note that `tape.gradient` works with a list as well (w, b).\n",
    "        dloss_dw, dloss_db = tape.gradient(loss, [w, b])\n",
    "    w.assign_sub(learning_rate * dloss_dw)\n",
    "    b.assign_sub(learning_rate * dloss_db)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some artificial data to demonstrate our model:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data.\n",
    "indices = np.random.permutation(len(features))\n",
    "features = features[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "# Create a tf.data.Dataset object for easy batched iteration\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(256)\n",
    "\n",
    "for epoch in range(5):\n",
    "    for step, (x, y) in enumerate(dataset):\n",
    "        loss = train_on_batch(x, y)\n",
    "    print('Epoch %d: last batch loss = %.4f' % (epoch, float(loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = compute_predictions(features)\n",
    "plt.scatter(features[:, 0], features[:, 1], c=predictions[:, 0] > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making it fast with tf.function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "for epoch in range(20):\n",
    "    for step, (x, y) in enumerate(dataset):\n",
    "        loss = train_on_batch(x, y)\n",
    "t_end = time.time() - t0\n",
    "print('Time per epoch: %.3f s' % (t_end / 20,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compile the training function into a static graph. Literally all we need to do is add the tf.function decorator on it:\n",
    "\n",
    "<img src=\"../images/tf2.0/autograph.png\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_on_batch(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = compute_predictions(x)\n",
    "        loss = compute_loss(y, predictions)\n",
    "        dloss_dw, dloss_db = tape.gradient(loss, [w, b])\n",
    "    w.assign_sub(learning_rate * dloss_dw)\n",
    "    b.assign_sub(learning_rate * dloss_db)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "for epoch in range(20):\n",
    "    for step, (x, y) in enumerate(dataset):\n",
    "        loss = train_on_batch(x, y)\n",
    "t_end = time.time() - t0\n",
    "print('Time per epoch: %.3f s' % (t_end / 20,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# The base `Layer` class\n",
    "\n",
    "\n",
    "The first class you need to know is [Layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer). Pretty much everything in Keras derives from it.\n",
    "\n",
    "A Layer encapsulates a state (weights) and some computation (defined in the `call` method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    \"\"\"y = w.x + b\"\"\"\n",
    "\n",
    "    def __init__(self, units=32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "                               initializer='random_normal',\n",
    "                               trainable=True, name='W')\n",
    "        self.b = self.add_weight(shape=(self.units,),\n",
    "                               initializer='random_normal',\n",
    "                               trainable=True, name='b')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate our lazy layer.\n",
    "linear_layer = Linear(4)\n",
    "\n",
    "# This will call `build(input_shape)` and create the weights.\n",
    "y = linear_layer(tf.ones((2, 2)))\n",
    "assert len(linear_layer.weights) == 2\n",
    "linear_layer.variables, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's reuse the Linear class\n",
    "# with a `build` method that we defined above.\n",
    "\n",
    "class MLP(Layer):\n",
    "    \"\"\"Simple stack of Linear layers.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear_1 = Linear(32)\n",
    "        self.linear_2 = Linear(32)\n",
    "        self.linear_3 = Linear(10)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.linear_1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return self.linear_3(x)\n",
    "\n",
    "mlp = MLP()\n",
    "\n",
    "# The first call to the `mlp` object will create the weights.\n",
    "y = mlp(tf.ones(shape=(3, 64)))\n",
    "\n",
    "# Weights are recursively tracked.\n",
    "assert len(mlp.weights) == 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x.name for x in mlp.variables]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The training argument in call\n",
    "\n",
    "Some layers, in particular the BatchNormalization layer and the Dropout layer, have different behaviors during training and inference. For such layers, it is standard practice to expose a training (boolean) argument in the call method.\n",
    "\n",
    "By exposing this argument in call, you enable the built-in training and evaluation loops (e.g. fit) to correctly use the layer in training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "class Dropout(Layer):\n",
    "\n",
    "    def __init__(self, rate):\n",
    "        super(Dropout, self).__init__()\n",
    "        self.rate = rate\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        if training:\n",
    "            return tf.nn.dropout(inputs, rate=self.rate)\n",
    "        return inputs\n",
    "\n",
    "class MLPWithDropout(Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MLPWithDropout, self).__init__()\n",
    "        self.linear_1 = Linear(32)\n",
    "        self.dropout = Dropout(0.5)\n",
    "        self.linear_3 = Linear(10)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.linear_1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        return self.linear_3(x)\n",
    "    \n",
    "mlp = MLPWithDropout()\n",
    "y_train = mlp(tf.ones((2, 2)), training=True)\n",
    "y_test = mlp(tf.ones((2, 2)), training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use an `Input` object to describe the shape and dtype of the inputs.\n",
    "# This is the deep learning equivalent of *declaring a type*.\n",
    "# The shape argument is per-sample; it does not include the batch size.\n",
    "# The functional API focused on defining per-sample transformations.\n",
    "# The model we create will automatically batch the per-sample transformations,\n",
    "# so that it can be called on batches of data.\n",
    "inputs = tf.keras.Input(shape=(16,))\n",
    "\n",
    "# We call layers on these \"type\" objects\n",
    "# and they return updated types (new shapes/dtypes).\n",
    "x = Linear(32)(inputs) # We are reusing the Linear layer we defined earlier.\n",
    "x = Dropout(0.5)(x) # We are reusing the Dropout layer we defined earlier.\n",
    "outputs = Linear(10)(x)\n",
    "\n",
    "# A functional `Model` can be defined by specifying inputs and outputs.\n",
    "# A model is itself a layer like any other.\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# A functional model already has weights, before being called on any data.\n",
    "# That's because we defined its input shape in advance (in `Input`).\n",
    "assert len(model.weights) == 4\n",
    "\n",
    "# Let's call our model on some data.\n",
    "y = model(tf.ones((2, 16)))\n",
    "assert y.shape == (2, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
