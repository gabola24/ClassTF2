{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Datasets Types\n",
    "\n",
    "https://www.tensorflow.org/guide/data#dataset_structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **from tensors:** Creates a Dataset with a single element, comprising the given tensors.\n",
    "* **from tensor slices:** Creates a Dataset whose elements are slices of the given tensors.\n",
    "* **from tfrecord:** A Dataset comprising records from one or more TFRecord files.\n",
    "* **from textline:** A Dataset comprising lines from one or more text files.\n",
    "* **from generator:** Creates a Dataset whose elements are generated by generator.\n",
    "\n",
    "```python\n",
    "tf.data.Dataset.from_tensors()\n",
    "tf.data.Dataset.from_tensor_slices()\n",
    "tf.data.Dataset.from_generator()\n",
    "tf.data.TFRecordDataset()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1.1 From Tensors](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "data = np.arange(10)\n",
    "dataset = tf.data.Dataset.from_tensors(data)\n",
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1.2 From tensor slices](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.arange(10)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1.3 From TextLine](https://www.tensorflow.org/api_docs/python/tf/data/TextLineDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./test.txt', 'w') as f:\n",
    "    f.writelines(np.array2string(np.random.rand(10), separator=',' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TextLineDataset(['./test.txt']) #note the input is a list of files\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in dataset.take(20):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm ./test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1.4 From Generator](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator():\n",
    "    for i in range(10):\n",
    "        yield 2*i\n",
    "    \n",
    "dataset = tf.data.Dataset.from_generator(generator, (tf.int32))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in dataset.take(5):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1.5 From tfrecords](https://www.tensorflow.org/tutorials/load_data/tfrecord#top_of_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "data_arr = [\n",
    "    {\n",
    "        'int_data': 108,\n",
    "        'float_data': 2.45,\n",
    "        'str_data': 'String 100',\n",
    "        'float_list_data': [256.78, 13.9]\n",
    "    },\n",
    "    {\n",
    "        'int_data': 37,\n",
    "        'float_data': 84.3,\n",
    "        'str_data': 'String 200',\n",
    "        'float_list_data': [1.34, 843.9, 65.22]\n",
    "    }\n",
    "]\n",
    "\n",
    "def get_example_object(data_record):\n",
    "    # Convert individual data into a list of int64 or float or bytes\n",
    "    int_list1 = tf.train.Int64List(value = [data_record['int_data']])\n",
    "    float_list1 = tf.train.FloatList(value = [data_record['float_data']])\n",
    "    # Convert string data into list of bytes\n",
    "    str_list1 = tf.train.BytesList(value = [data_record['str_data'].encode('utf-8')])\n",
    "    float_list2 = tf.train.FloatList(value = data_record['float_list_data'])\n",
    "\n",
    "    # Create a dictionary with above lists individually wrapped in Feature\n",
    "    feature_key_value_pair = {\n",
    "        'int_list1': tf.train.Feature(int64_list = int_list1),\n",
    "        'float_list1': tf.train.Feature(float_list = float_list1),\n",
    "        'str_list1': tf.train.Feature(bytes_list = str_list1),\n",
    "        'float_list2': tf.train.Feature(float_list = float_list2)\n",
    "    }\n",
    "\n",
    "    # Create Features object with above feature dictionary\n",
    "    features = tf.train.Features(feature = feature_key_value_pair)\n",
    "\n",
    "    # Create Example object with features\n",
    "    example = tf.train.Example(features = features)\n",
    "    return example\n",
    "\n",
    "with tf.io.TFRecordWriter('./example.tfrecord') as tfwriter:\n",
    "    # Iterate through all records\n",
    "    for data_record in data_arr:\n",
    "        example = get_example_object(data_record)\n",
    "        # Append each example into tfrecord\n",
    "        tfwriter.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TFRecordDataset(['./example.tfrecord'])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for raw_record in dataset.take(1):\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(raw_record.numpy())\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm ./example.tfrecord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1.6 From other formats](https://github.com/tensorflow/io#tensorflow-io) (BigQuery, GCS, hdf5, AVRO, parquet, etc.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# 2. Datasets Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Batching: \n",
    "Combines consecutive elements of this dataset into batches.\n",
    "\n",
    "\n",
    "<img src=\"../images/2_datasets/batch.jpeg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.arange(8)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "dataset = dataset.batch(3)\n",
    "for x in dataset:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Repeat\n",
    "Repeats this dataset count times.\n",
    "\n",
    "<img src=\"../images/2_datasets/repeat.jpeg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.arange(4)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "dataset = dataset.batch(4).repeat(2)\n",
    "for x in dataset:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2.3 Suffle](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle)\n",
    "Randomly shuffles the elements of this dataset.\n",
    "\n",
    "<img src=\"../images/2_datasets/shuffle.jpeg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.arange(8)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "dataset = dataset.shuffle(4)\n",
    "for x in dataset:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in dataset.batch(8):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Map\n",
    "Maps map_func across the elements of this dataset.\n",
    "\n",
    "<img src=\"../images/2_datasets/map.jpeg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.arange(8)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "dataset = dataset.batch(8).map(lambda x: x+1)\n",
    "for x in dataset:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/2_datasets/map_2.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.arange(8)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "dataset = dataset.batch(8).map(lambda x: x+1, num_parallel_calls= tf.data.experimental.AUTOTUNE)\n",
    "for x in dataset:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Filter\n",
    "Filters this dataset according to predicate.\n",
    "\n",
    "<img src=\"../images/2_datasets/filter.jpeg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.arange(8)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "dataset = dataset.filter(lambda x: x%2==0).batch(4)\n",
    "for x in dataset:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.6 Prefetch\n",
    "Creates a Dataset that prefetches elements from this dataset.\n",
    "\n",
    "<img src=\"../images/2_datasets/prefetch.png\" width=\"600\">\n",
    "<img src=\"../images/2_datasets/prefetch_1.png\" width=\"600\">\n",
    "<img src=\"../images/2_datasets/prefetch_2.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.arange(8)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "dataset = dataset.filter(lambda x: x%2==0).map(lambda y: y+1).batch(4)\n",
    "dataset = dataset.prefetch(3)\n",
    "for x in dataset:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Performance\n",
    "\n",
    "Use [the official guide](https://www.tensorflow.org/guide/data_performance#optimize_performance) below is the summary.\n",
    "\n",
    "* Use the prefetch transformation to overlap the work of a producer and consumer. In particular, we recommend adding prefetch to the end of your input pipeline to overlap the transformations performed on the CPU with the training done on the accelerator. Either manually tuning the buffer size, or using tf.data.experimental.AUTOTUNE to delegate the decision to the tf.data runtime.\n",
    "* Parallelize the map transformation by setting the num_parallel_calls argument. Either manually tuning the level of parallelism, or using tf.data.experimental.AUTOTUNE to delegate the decision to the tf.data runtime.\n",
    "* If you are working with data stored remotely and / or requiring deserialization, we recommend using the interleave transformation to parallelize the reading (and deserialization) of data from different files.\n",
    "* Vectorize cheap user-defined functions passed in to the map transformation to amortize the overhead associated with scheduling and executing the function.\n",
    "* If your data can fit into memory, use the cache transformation to cache it in memory during the first epoch, so that subsequent epochs can avoid the overhead associated with reading, parsing, and transforming it.\n",
    "* If your pre-processing increases the size of your data, we recommend applying the interleave, prefetch, and shuffle first (if possible) to reduce memory usage.\n",
    "* We recommend applying the shuffle transformation before the repeat transformation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
